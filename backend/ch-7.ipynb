{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter-7\n",
    "\n",
    "# Introduction to Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'requests'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrequests\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbs4\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BeautifulSoup\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'requests'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url=\"https://www.netflix.com/tudum/top10\"\n",
    "response=requests.get(url)\n",
    "soup=BeautifulSoup(response.text,'html.parser')\n",
    "movie_top10_title=[]\n",
    "movie_top10_images=[]\n",
    "link2 = soup.find_all(\"div\",class_= \"css-1h3btjz\")\n",
    "for m in link2:\n",
    "    link3=m.find_all(\"img\")\n",
    "    # print(link3)\n",
    "    for n in link3:\n",
    "        # print(n)\n",
    "        title=n.get(\"alt\")\n",
    "        # print(title)\n",
    "        movie_top10_title.append(title)\n",
    "\n",
    "images=soup.find_all(\"div\",class_=\"css-1humui4 eespg911\")\n",
    "for i in images:\n",
    "    image=i.get(\"style\").replace(\"background-image:url(\",'').split(\", \")[0]\n",
    "    movie_top10_images.append(image)\n",
    "    print(image)\n",
    "top_10={\n",
    "    \"Movie Title\":movie_top10_title,\n",
    "    \"Movie_images\":movie_top10_images\n",
    "}\n",
    "df=pd.DataFrame(top_10)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "url=\"https://www.netflix.com/tudum/topics/trending\"\n",
    "response=requests.get(url)\n",
    "soup=BeautifulSoup(response.text,'html.parser')\n",
    "\n",
    "movie_trending_headline=[]\n",
    "movie_trending_images=[]\n",
    "movie_trending_eyebrow=[]\n",
    "movie_trending_subheadline=[]\n",
    "movie_trending_by_line=[]\n",
    "movie_trending_article_date=[]\n",
    "movie_article_data=[]\n",
    "\n",
    "trending_movie_data= soup.find_all(\"ul\",class_= \"fade-in-articles css-1fv4fhq es0eqmb0\")\n",
    "subheadline_tag = soup.find_all('div', class_='sub-headline css-u6l51d esmf2q80')\n",
    "By_line=soup.find_all(\"div\",class_=\"css-19pirhg esmf2q85\")\n",
    "\n",
    "for a in By_line:\n",
    "    author=a.text.strip()\n",
    "    if author:\n",
    "        movie_trending_by_line.append(author)\n",
    "        # print(author)\n",
    "    else:\n",
    "        movie_trending_by_line.append(\"No Author Mentioned\")\n",
    "        # print(\"None\")\n",
    "    # print(a.text.strip())\n",
    "\n",
    "\n",
    "# Find the span with the specific class\n",
    "\n",
    "count=0\n",
    "# count_headline\n",
    "for j in subheadline_tag:\n",
    "    subheading_movie=j.find(\"span\")\n",
    "    # print(subheading_movie.get_text(strip=True))\n",
    "    count=count+1\n",
    "    if count==5:\n",
    "        continue\n",
    "    else:    \n",
    "        movie_trending_subheadline.append(subheading_movie.get_text(strip=True))\n",
    "        # print(j.get_text(strip=True))\n",
    "for m in trending_movie_data:\n",
    "    trending_movie_article_link=m.find_all(\"a\",class_=\"css-13y5uoc\")\n",
    "    trending_movie_image_div=m.find_all(\"div\",class_=\"css-1d3w5wq\")\n",
    "    trending_movie_headline=m.find_all(\"div\",class_=\"css-1uafqei esmf2q84\")\n",
    "\n",
    "    for k in trending_movie_headline:\n",
    "        heading_movie=k.find(\"a\",class_=\"css-13y5uoc\").text\n",
    "        eyebrow_movie=k.find(\"div\").text\n",
    "        By_line=k.find(\"div\",class_=\"css-19pirhg esmf2q85\")\n",
    "        Date=k.find(\"div\",class_=\"css-1bmnxg7\").text\n",
    "        if By_line:\n",
    "            author=By_line.text.strip()\n",
    "            movie_trending_by_line.append(author)\n",
    "        else:\n",
    "            movie_trending_by_line.append(\"No Author Mentioned\")\n",
    "        # count=count+1\n",
    "        movie_trending_article_date.append(Date)\n",
    "        movie_trending_headline.append(heading_movie)\n",
    "        movie_trending_eyebrow.append(eyebrow_movie)\n",
    "        # movie_trending_by_line.append(By_line.get_text(strip=True))\n",
    "        # print(By_line)\n",
    "\n",
    "    for m in trending_movie_data:\n",
    "        trending_movie_article_link = m.find_all(\"a\", class_=\"css-13y5uoc\")\n",
    "   \n",
    "    for b in trending_movie_article_link:\n",
    "        article_link = b.get(\"href\")\n",
    "        \n",
    "        # Correctly join relative link with base URL\n",
    "        full_article_url = urljoin(url, article_link)\n",
    "        print(f\"Fetching: {full_article_url}\")\n",
    "\n",
    "        response1 = requests.get(full_article_url)\n",
    "        article_soup = BeautifulSoup(response1.text, 'html.parser')\n",
    "\n",
    "        paragraphs = article_soup.find_all(\"p\")\n",
    "        article_text = \" \".join([p.get_text(strip=True) for p in paragraphs])\n",
    "\n",
    "        movie_article_data.append({\n",
    "            \"title\": b.get_text(strip=True),\n",
    "            \"url\": full_article_url,\n",
    "            \"article_link\":article_link,\n",
    "            \"content\": article_text\n",
    "        })\n",
    "\n",
    "    for n in trending_movie_image_div:\n",
    "        image_trending=n.find_all(\"img\",class_=\"css-64wllk\")\n",
    "        for i in image_trending:\n",
    "            movie_image=i.get(\"src\")\n",
    "            movie_trending_images.append(movie_image)\n",
    "\n",
    "    print()\n",
    "print(len(movie_trending_article_date))\n",
    "print(len(movie_trending_images))\n",
    "print(len(movie_trending_eyebrow))\n",
    "print(len(movie_trending_subheadline))\n",
    "print(len(movie_trending_headline))\n",
    "print(len(movie_trending_by_line))\n",
    "print(len(movie_article_data))\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"Article Headline\": movie_trending_headline,\n",
    "    \"Article SubHeadline\":movie_trending_subheadline,\n",
    "    \"Movie Article Image\": movie_trending_images,\n",
    "    \"Eyebrow Headline\": movie_trending_eyebrow,\n",
    "    \"By\": movie_trending_by_line,\n",
    "    \"Article Date\":movie_trending_article_date,\n",
    "    \"Article Data\":movie_article_data\n",
    "})\n",
    "# # # print(df)\n",
    "df.to_csv(\"new_trending_movies_articles_data.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(\"CSV file saved as trending_movie_data.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "url=\"https://www.netflix.com/tudum/topics/trending\"\n",
    "response=requests.get(url)\n",
    "soup=BeautifulSoup(response.text,'html.parser')\n",
    "\n",
    "movie_trending_headline=[]\n",
    "movie_trending_images=[]\n",
    "movie_trending_eyebrow=[]\n",
    "movie_trending_subheadline=[]\n",
    "movie_trending_by_line=[]\n",
    "movie_trending_article_date=[]\n",
    "movie_article_content=[]\n",
    "movie_article_link=[]\n",
    "movie_article_real_url=[]\n",
    "\n",
    "\n",
    "trending_movie_data= soup.find_all(\"ul\",class_= \"fade-in-articles css-1fv4fhq es0eqmb0\")\n",
    "subheadline_tag = soup.find_all('div', class_='sub-headline css-u6l51d esmf2q80')\n",
    "By_line=soup.find_all(\"div\",class_=\"css-19pirhg esmf2q85\")\n",
    "\n",
    "# for a in By_line:\n",
    "#     author=a.text.strip()\n",
    "#     if author:\n",
    "#         movie_trending_by_line.append(author)\n",
    "#         # print(author)\n",
    "#     else:\n",
    "#         movie_trending_by_line.append(\"No Author Mentioned\")\n",
    "#         # print(\"None\")\n",
    "#     # print(a.text.strip())\n",
    "\n",
    "\n",
    "# Find the span with the specific class\n",
    "\n",
    "count=0\n",
    "# count_headline\n",
    "for j in subheadline_tag:\n",
    "    subheading_movie=j.find(\"span\")\n",
    "    # print(subheading_movie.get_text(strip=True))\n",
    "    count=count+1\n",
    "    if count==5:\n",
    "        continue\n",
    "    else:    \n",
    "        movie_trending_subheadline.append(subheading_movie.get_text(strip=True))\n",
    "        # print(j.get_text(strip=True))\n",
    "for m in trending_movie_data:\n",
    "    trending_movie_article_link=m.find_all(\"a\",class_=\"css-13y5uoc\")\n",
    "    trending_movie_image_div=m.find_all(\"div\",class_=\"css-1d3w5wq\")\n",
    "    trending_movie_headline=m.find_all(\"div\",class_=\"css-1uafqei esmf2q84\")\n",
    "\n",
    "    for k in trending_movie_headline:\n",
    "        heading_movie=k.find(\"a\",class_=\"css-13y5uoc\").text\n",
    "        eyebrow_movie=k.find(\"div\").text\n",
    "        By_line=k.find(\"div\",class_=\"css-19pirhg esmf2q85\")\n",
    "        Date=k.find(\"div\",class_=\"css-1bmnxg7\").text\n",
    "        if By_line:\n",
    "            author=By_line.text.strip()\n",
    "            movie_trending_by_line.append(author)\n",
    "        else:\n",
    "            movie_trending_by_line.append(\"No Author Mentioned\")\n",
    "        # count=count+1\n",
    "        movie_trending_article_date.append(Date)\n",
    "        movie_trending_headline.append(heading_movie)\n",
    "        movie_trending_eyebrow.append(eyebrow_movie)\n",
    "        # movie_trending_by_line.append(By_line.get_text(strip=True))\n",
    "        # print(By_line)\n",
    "\n",
    "\n",
    "    for n in trending_movie_image_div:\n",
    "        image_trending=n.find_all(\"img\",class_=\"css-64wllk\")\n",
    "        for i in image_trending:\n",
    "            movie_image=i.get(\"src\")\n",
    "            movie_trending_images.append(movie_image)\n",
    "\n",
    "    # print()\n",
    "print(len(movie_trending_article_date))\n",
    "print(len(movie_trending_images))\n",
    "print(len(movie_trending_eyebrow))\n",
    "print(len(movie_trending_subheadline))\n",
    "print(len(movie_trending_headline))\n",
    "print(len(movie_trending_by_line))\n",
    "for m in trending_movie_data:\n",
    "    trending_movie_article_link = m.find_all(\"a\", class_=\"css-13y5uoc\")\n",
    "   \n",
    "    for b in trending_movie_article_link:\n",
    "        article_link = b.get(\"href\")\n",
    "        \n",
    "        # Correctly join relative link with base URL\n",
    "        full_article_url = urljoin(url, article_link)\n",
    "        print(f\"Fetching: {full_article_url}\")\n",
    "\n",
    "        response1 = requests.get(full_article_url)\n",
    "        article_soup = BeautifulSoup(response1.text, 'html.parser')\n",
    "\n",
    "        paragraphs = article_soup.find_all(\"p\")\n",
    "        article_text = \" \".join([p.get_text(strip=True) for p in paragraphs])\n",
    "\n",
    "        movie_article_content.append(article_text)\n",
    "        movie_article_link.append(article_link)\n",
    "        movie_article_real_url.append(full_article_url)\n",
    "        #     \"title\": b.get_text(strip=True),\n",
    "        #     \"url\": full_article_url,\n",
    "        #     \"article_link\":article_link,\n",
    "print(len(movie_article_content))\n",
    "print(len(movie_article_link))\n",
    "print(len(movie_article_real_url))\n",
    "# Save to CSV\n",
    "# # df = pd.DataFrame(movie_article_data)\n",
    "# df.to_csv(\"trending_movies_article_data.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "# print(f\"✅ Saved {len(df)} articles to trending_movies_article_data.csv\")\n",
    "\n",
    "\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"Article Headline\": movie_trending_headline,\n",
    "    \"Article SubHeadline\":movie_trending_subheadline,\n",
    "    \"Movie Article Image\": movie_trending_images,\n",
    "    \"Eyebrow Headline\": movie_trending_eyebrow,\n",
    "    \"By\": movie_trending_by_line,\n",
    "    \"Article Date\":movie_trending_article_date,\n",
    "    \"Article Link\": movie_article_link,\n",
    "    \"Article Real URL\": movie_article_real_url,\n",
    "    \"Article Content\":movie_article_content,\n",
    "})\n",
    "# # # print(df)\n",
    "df.to_csv(\"new_trending_movies_with_articles_data.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(\"CSV file saved as trending_movie_data.csv\")\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://www.netflix.com/tudum/topics/trending\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "all_articles = []  # ✅ main storage list\n",
    "\n",
    "# Find trending movie sections\n",
    "trending_movie_data = soup.find_all(\"ul\", class_=\"fade-in-articles css-1fv4fhq es0eqmb0\")\n",
    "\n",
    "for m in trending_movie_data:\n",
    "    trending_movie_article_link = m.find_all(\"a\", class_=\"css-13y5uoc\")\n",
    "   \n",
    "    for b in trending_movie_article_link:\n",
    "        article_link = b.get(\"href\")\n",
    "        \n",
    "        # Correctly join relative link with base URL\n",
    "        full_article_url = urljoin(url, article_link)\n",
    "        print(f\"Fetching: {full_article_url}\")\n",
    "\n",
    "        response1 = requests.get(full_article_url)\n",
    "        article_soup = BeautifulSoup(response1.text, 'html.parser')\n",
    "\n",
    "        paragraphs = article_soup.find_all(\"p\")\n",
    "        article_text = \" \".join([p.get_text(strip=True) for p in paragraphs])\n",
    "\n",
    "        all_articles.append({\n",
    "            \"title\": b.get_text(strip=True),\n",
    "            \"url\": full_article_url,\n",
    "            \"article_link\":article_link,\n",
    "            \"content\": article_text\n",
    "        })\n",
    "\n",
    "# Save to CSV\n",
    "df = pd.DataFrame(all_articles)\n",
    "df.to_csv(\"trending_movies_article_data.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"✅ Saved {len(df)} articles to trending_movies_article_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://www.netflix.com/tudum/topics/trending\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "all_articles = []  # ✅ main storage list\n",
    "\n",
    "# Find trending movie sections\n",
    "trending_movie_data = soup.find_all(\"ul\", class_=\"fade-in-articles css-1fv4fhq es0eqmb0\")\n",
    "trending_movie_article_link = soup.find(\"a\", class_=\"css-13y5uoc\")\n",
    "article_link = trending_movie_article_link.get(\"href\")\n",
    "full_article_url = urljoin(url, article_link)\n",
    "print(f\"Fetching: {full_article_url}\")\n",
    "\n",
    "response1 = requests.get(full_article_url)\n",
    "article_soup = BeautifulSoup(response1.text, 'html.parser')\n",
    "\n",
    "paragraphs = article_soup.find_all(\"p\")\n",
    "# article_text = \" \".join([p.get_text(strip=True) for p in paragraphs])\n",
    "# print(article_text)\n",
    "article_text=\"\"\n",
    "for i in paragraphs:\n",
    "    article_text=article_text.join(i.get_text(strip=True))\n",
    "    article_text=article_text+\"/n\"\n",
    "    print(article_text)\n",
    "    \n",
    "#         movie_article_content.append(article_text)\n",
    "#         movie_article_link.append(article_link)\n",
    "#         movie_article_real_url.append(full_article_url)\n",
    "# # Save to CSV\n",
    "# df = pd.DataFrame(all_articles)\n",
    "# df.to_csv(\"trending_movies_article_data.csv\", index=False, encoding=\"utf-8\")\n",
    "# df\n",
    "# print(f\"✅ Saved {len(df)} articles to trending_movies_article_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://www.netflix.com/tudum/articles/wednesday-season-2-character-cast-guide\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "article_data=soup.find"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url=\"https://www.netflix.com/tudum/topics/trending\"\n",
    "response=requests.get(url)\n",
    "soup=BeautifulSoup(response.text,'html.parser')\n",
    "\n",
    "movie_trending_headline=[]\n",
    "movie_trending_images=[]\n",
    "movie_trending_eyebrow=[]\n",
    "movie_trending_subheadline=[]\n",
    "movie_trending_by_line=[]\n",
    "movie_trending_article_date=[]\n",
    "\n",
    "trending_movie_data= soup.find_all(\"ul\",class_= \"fade-in-articles css-1fv4fhq es0eqmb0\")\n",
    "subheadline_tag = soup.find_all('div', class_='sub-headline css-u6l51d esmf2q80')\n",
    "# By_line=soup.find_all(\"div\",class_=\"css-19pirhg esmf2q85\")\n",
    "\n",
    "# for a in By_line:\n",
    "#     author=a.text.strip()\n",
    "#     if author:\n",
    "#         movie_trending_by_line.append(author)\n",
    "#         print(author)\n",
    "#     else:\n",
    "#         movie_trending_by_line.append(\"No Author Mentioned\")\n",
    "#         print(\"None\")\n",
    "    # print(a.text.strip())\n",
    "\n",
    "\n",
    "# Find the span with the specific class\n",
    "\n",
    "count=0\n",
    "# count_headline\n",
    "for j in subheadline_tag:\n",
    "    subheading_movie=j.find(\"span\")\n",
    "    # print(subheading_movie.get_text(strip=True))\n",
    "    count=count+1\n",
    "    if count==5:\n",
    "        continue\n",
    "    else:    \n",
    "        movie_trending_subheadline.append(subheading_movie.get_text(strip=True))\n",
    "        # print(j.get_text(strip=True))\n",
    "for m in trending_movie_data:\n",
    "    trending_movie_image_div=m.find_all(\"div\",class_=\"css-1d3w5wq\")\n",
    "    trending_movie_headline=m.find_all(\"div\",class_=\"css-1uafqei esmf2q84\")\n",
    "\n",
    "    for k in trending_movie_headline:\n",
    "        heading_movie=k.find(\"a\",class_=\"css-13y5uoc\").text\n",
    "        eyebrow_movie=k.find(\"div\").text\n",
    "        By_line=k.find(\"div\",class_=\"css-19pirhg esmf2q85\")\n",
    "        Date=k.find(\"div\",class_=\"css-1bmnxg7\").text\n",
    "        if By_line:\n",
    "            author=By_line.text.strip()\n",
    "            movie_trending_by_line.append(author)\n",
    "        else:\n",
    "            movie_trending_by_line.append(\"No Author Mentioned\")\n",
    "        # count=count+1\n",
    "        movie_trending_article_date.append(Date)\n",
    "        movie_trending_headline.append(heading_movie)\n",
    "        movie_trending_eyebrow.append(eyebrow_movie)\n",
    "        # movie_trending_by_line.append(By_line.get_text(strip=True))\n",
    "        # print(By_line)\n",
    "\n",
    "\n",
    "\n",
    "    for n in trending_movie_image_div:\n",
    "        image_trending=n.find_all(\"img\",class_=\"css-64wllk\")\n",
    "        for i in image_trending:\n",
    "            movie_image=i.get(\"src\")\n",
    "            movie_trending_images.append(movie_image)\n",
    "\n",
    "    print()\n",
    "print(len(movie_trending_article_date))\n",
    "print(len(movie_trending_images))\n",
    "print(len(movie_trending_eyebrow))\n",
    "print(len(movie_trending_subheadline))\n",
    "print(len(movie_trending_headline))\n",
    "print(len(movie_trending_by_line))\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"Article Headline\": movie_trending_headline,\n",
    "    \"Article SubHeadline\":movie_trending_subheadline,\n",
    "    \"Movie Article Image\": movie_trending_images,\n",
    "    \"Eyebrow Headline\": movie_trending_eyebrow,\n",
    "    \"By\": movie_trending_by_line,\n",
    "    \"Article Date\":movie_trending_article_date\n",
    "})\n",
    "# # print(df)\n",
    "# df.to_csv(\"trending_movies_data1.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "# print(\"CSV file saved as trending_movie_data.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url=\"https://www.netflix.com/tudum/topics/what-to-watch\"\n",
    "response=requests.get(url)\n",
    "soup=BeautifulSoup(response.text,'html.parser')\n",
    "\n",
    "movie_trending_headline=[]\n",
    "movie_trending_images=[]\n",
    "movie_trending_eyebrow=[]\n",
    "movie_trending_subheadline=[]\n",
    "movie_trending_by_line=[]\n",
    "movie_trending_article_date=[]\n",
    "\n",
    "trending_movie_data= soup.find_all(\"ul\",class_= \"fade-in-articles css-1fv4fhq es0eqmb0\")\n",
    "subheadline_tag = soup.find_all('div', class_='sub-headline css-u6l51d esmf2q80')\n",
    "# By_line=soup.find_all(\"div\",class_=\"css-19pirhg esmf2q85\")\n",
    "\n",
    "# for a in By_line:\n",
    "#     author=a.text.strip()\n",
    "#     if author:\n",
    "#         movie_trending_by_line.append(author)\n",
    "#         print(author)\n",
    "#     else:\n",
    "#         movie_trending_by_line.append(\"No Author Mentioned\")\n",
    "#         print(\"None\")\n",
    "    # print(a.text.strip())\n",
    "\n",
    "\n",
    "# Find the span with the specific class\n",
    "\n",
    "count=0\n",
    "# count_headline\n",
    "for j in subheadline_tag:\n",
    "    subheading_movie=j.find(\"span\")\n",
    "    # print(subheading_movie.get_text(strip=True))\n",
    "    count=count+1\n",
    "    if count==0:\n",
    "        continue\n",
    "    else:    \n",
    "        movie_trending_subheadline.append(subheading_movie.get_text(strip=True))\n",
    "        # print(j.get_text(strip=True))\n",
    "for m in trending_movie_data:\n",
    "    trending_movie_image_div=m.find_all(\"div\",class_=\"css-1d3w5wq\")\n",
    "    trending_movie_headline=m.find_all(\"div\",class_=\"css-1uafqei esmf2q84\")\n",
    "\n",
    "    for k in trending_movie_headline:\n",
    "        heading_movie=k.find(\"a\",class_=\"css-13y5uoc\").text\n",
    "        eyebrow_movie=k.find(\"div\").text\n",
    "        By_line=k.find(\"div\",class_=\"css-19pirhg esmf2q85\")\n",
    "        Date=k.find(\"div\",class_=\"css-1bmnxg7\").text\n",
    "        if By_line:\n",
    "            author=By_line.text.strip()\n",
    "            movie_trending_by_line.append(author)\n",
    "        else:\n",
    "            movie_trending_by_line.append(\"No Author Mentioned\")\n",
    "        # count=count+1\n",
    "        movie_trending_article_date.append(Date)\n",
    "        movie_trending_headline.append(heading_movie)\n",
    "        movie_trending_eyebrow.append(eyebrow_movie)\n",
    "        # movie_trending_by_line.append(By_line.get_text(strip=True))\n",
    "        # print(By_line)\n",
    "\n",
    "\n",
    "\n",
    "    for n in trending_movie_image_div:\n",
    "        image_trending=n.find_all(\"img\",class_=\"css-64wllk\")\n",
    "        for i in image_trending:\n",
    "            movie_image=i.get(\"src\")\n",
    "            movie_trending_images.append(movie_image)\n",
    "\n",
    "    print()\n",
    "print(len(movie_trending_article_date))\n",
    "print(len(movie_trending_images))\n",
    "print(len(movie_trending_eyebrow))\n",
    "print(len(movie_trending_subheadline))\n",
    "print(len(movie_trending_headline))\n",
    "print(len(movie_trending_by_line))\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"Article Headline\": movie_trending_headline,\n",
    "    \"Article SubHeadline\":movie_trending_subheadline,\n",
    "    \"Movie Article Image\": movie_trending_images,\n",
    "    \"Eyebrow Headline\": movie_trending_eyebrow,\n",
    "    \"By\": movie_trending_by_line,\n",
    "    \"Article Date\":movie_trending_article_date\n",
    "})\n",
    "# # print(df)\n",
    "# df.to_csv(\"trending_movies_data1.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "# print(\"CSV file saved as trending_movie_data.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url=\"https://www.netflix.com/tudum/topics/tv-shows\"\n",
    "response=requests.get(url)\n",
    "soup=BeautifulSoup(response.text,'html.parser')\n",
    "\n",
    "movie_trending_headline=[]\n",
    "movie_trending_images=[]\n",
    "movie_trending_eyebrow=[]\n",
    "movie_trending_subheadline=[]\n",
    "movie_trending_by_line=[]\n",
    "movie_trending_article_date=[]\n",
    "\n",
    "trending_movie_data= soup.find_all(\"ul\",class_= \"fade-in-articles css-1fv4fhq es0eqmb0\")\n",
    "subheadline_tag = soup.find_all('div', class_='sub-headline css-u6l51d esmf2q80')\n",
    "# By_line=soup.find_all(\"div\",class_=\"css-19pirhg esmf2q85\")\n",
    "\n",
    "# for a in By_line:\n",
    "#     author=a.text.strip()\n",
    "#     if author:\n",
    "#         movie_trending_by_line.append(author)\n",
    "#         print(author)\n",
    "#     else:\n",
    "#         movie_trending_by_line.append(\"No Author Mentioned\")\n",
    "#         print(\"None\")\n",
    "    # print(a.text.strip())\n",
    "\n",
    "\n",
    "# Find the span with the specific class\n",
    "\n",
    "count=0\n",
    "# count_headline\n",
    "for j in subheadline_tag:\n",
    "    subheading_movie=j.find(\"span\")\n",
    "    # print(subheading_movie.get_text(strip=True))\n",
    "    count=count+1\n",
    "    if count==5:\n",
    "        continue\n",
    "    else:    \n",
    "        movie_trending_subheadline.append(subheading_movie.get_text(strip=True))\n",
    "        # print(j.get_text(strip=True))\n",
    "for m in trending_movie_data:\n",
    "    trending_movie_image_div=m.find_all(\"div\",class_=\"css-1d3w5wq\")\n",
    "    trending_movie_headline=m.find_all(\"div\",class_=\"css-1uafqei esmf2q84\")\n",
    "\n",
    "    for k in trending_movie_headline:\n",
    "        heading_movie=k.find(\"a\",class_=\"css-13y5uoc\").text\n",
    "        eyebrow_movie=k.find(\"div\").text\n",
    "        By_line=k.find(\"div\",class_=\"css-19pirhg esmf2q85\")\n",
    "        Date=k.find(\"div\",class_=\"css-1bmnxg7\").text\n",
    "        if By_line:\n",
    "            author=By_line.text.strip()\n",
    "            movie_trending_by_line.append(author)\n",
    "        else:\n",
    "            movie_trending_by_line.append(\"No Author Mentioned\")\n",
    "        # count=count+1\n",
    "        movie_trending_article_date.append(Date)\n",
    "        movie_trending_headline.append(heading_movie)\n",
    "        movie_trending_eyebrow.append(eyebrow_movie)\n",
    "        # movie_trending_by_line.append(By_line.get_text(strip=True))\n",
    "        # print(By_line)\n",
    "\n",
    "\n",
    "\n",
    "    for n in trending_movie_image_div:\n",
    "        image_trending=n.find_all(\"img\",class_=\"css-64wllk\")\n",
    "        for i in image_trending:\n",
    "            movie_image=i.get(\"src\")\n",
    "            movie_trending_images.append(movie_image)\n",
    "\n",
    "    print()\n",
    "print(len(movie_trending_article_date))\n",
    "print(len(movie_trending_images))\n",
    "print(len(movie_trending_eyebrow))\n",
    "print(len(movie_trending_subheadline))\n",
    "print(len(movie_trending_headline))\n",
    "print(len(movie_trending_by_line))\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"Article Headline\": movie_trending_headline,\n",
    "    \"Article SubHeadline\":movie_trending_subheadline,\n",
    "    \"Movie Article Image\": movie_trending_images,\n",
    "    \"Eyebrow Headline\": movie_trending_eyebrow,\n",
    "    \"By\": movie_trending_by_line,\n",
    "    \"Article Date\":movie_trending_article_date\n",
    "})\n",
    "# # print(df)\n",
    "# df.to_csv(\"trending_movies_data1.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "# print(\"CSV file saved as trending_movie_data.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url=\"https://www.netflix.com/tudum/topics/movies\"\n",
    "response=requests.get(url)\n",
    "soup=BeautifulSoup(response.text,'html.parser')\n",
    "\n",
    "movie_trending_headline=[]\n",
    "movie_trending_images=[]\n",
    "movie_trending_eyebrow=[]\n",
    "movie_trending_subheadline=[]\n",
    "movie_trending_by_line=[]\n",
    "movie_trending_article_date=[]\n",
    "\n",
    "trending_movie_data= soup.find_all(\"ul\",class_= \"fade-in-articles css-1fv4fhq es0eqmb0\")\n",
    "subheadline_tag = soup.find_all('div', class_='sub-headline css-u6l51d esmf2q80')\n",
    "# By_line=soup.find_all(\"div\",class_=\"css-19pirhg esmf2q85\")\n",
    "\n",
    "# for a in By_line:\n",
    "#     author=a.text.strip()\n",
    "#     if author:\n",
    "#         movie_trending_by_line.append(author)\n",
    "#         print(author)\n",
    "#     else:\n",
    "#         movie_trending_by_line.append(\"No Author Mentioned\")\n",
    "#         print(\"None\")\n",
    "    # print(a.text.strip())\n",
    "\n",
    "\n",
    "# Find the span with the specific class\n",
    "\n",
    "count=0\n",
    "# count_headline\n",
    "for j in subheadline_tag:\n",
    "    subheading_movie=j.find(\"span\")\n",
    "    # print(subheading_movie.get_text(strip=True))\n",
    "    count=count+1\n",
    "    if count==5:\n",
    "        continue\n",
    "    else:    \n",
    "        movie_trending_subheadline.append(subheading_movie.get_text(strip=True))\n",
    "        # print(j.get_text(strip=True))\n",
    "for m in trending_movie_data:\n",
    "    trending_movie_image_div=m.find_all(\"div\",class_=\"css-1d3w5wq\")\n",
    "    trending_movie_headline=m.find_all(\"div\",class_=\"css-1uafqei esmf2q84\")\n",
    "\n",
    "    for k in trending_movie_headline:\n",
    "        heading_movie=k.find(\"a\",class_=\"css-13y5uoc\").text\n",
    "        eyebrow_movie=k.find(\"div\").text\n",
    "        By_line=k.find(\"div\",class_=\"css-19pirhg esmf2q85\")\n",
    "        Date=k.find(\"div\",class_=\"css-1bmnxg7\").text\n",
    "        if By_line:\n",
    "            author=By_line.text.strip()\n",
    "            movie_trending_by_line.append(author)\n",
    "        else:\n",
    "            movie_trending_by_line.append(\"No Author Mentioned\")\n",
    "        # count=count+1\n",
    "        movie_trending_article_date.append(Date)\n",
    "        movie_trending_headline.append(heading_movie)\n",
    "        movie_trending_eyebrow.append(eyebrow_movie)\n",
    "        # movie_trending_by_line.append(By_line.get_text(strip=True))\n",
    "        # print(By_line)\n",
    "\n",
    "\n",
    "\n",
    "    for n in trending_movie_image_div:\n",
    "        image_trending=n.find_all(\"img\",class_=\"css-64wllk\")\n",
    "        for i in image_trending:\n",
    "            movie_image=i.get(\"src\")\n",
    "            movie_trending_images.append(movie_image)\n",
    "\n",
    "    print()\n",
    "print(len(movie_trending_article_date))\n",
    "print(len(movie_trending_images))\n",
    "print(len(movie_trending_eyebrow))\n",
    "print(len(movie_trending_subheadline))\n",
    "print(len(movie_trending_headline))\n",
    "print(len(movie_trending_by_line))\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"Article Headline\": movie_trending_headline,\n",
    "    \"Article SubHeadline\":movie_trending_subheadline,\n",
    "    \"Movie Article Image\": movie_trending_images,\n",
    "    \"Eyebrow Headline\": movie_trending_eyebrow,\n",
    "    \"By\": movie_trending_by_line,\n",
    "    \"Article Date\":movie_trending_article_date\n",
    "})\n",
    "# # print(df)\n",
    "# df.to_csv(\"trending_movies_data1.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "# print(\"CSV file saved as trending_movie_data.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url=\"https://www.netflix.com/tudum\"\n",
    "response=requests.get(url)\n",
    "soup=BeautifulSoup(response.text,'html.parser')\n",
    "\n",
    "movie_trending_headline=[]\n",
    "movie_trending_images=[]\n",
    "movie_trending_eyebrow=[]\n",
    "movie_trending_subheadline=[]\n",
    "movie_trending_by_line=[]\n",
    "movie_trending_article_date=[]\n",
    "\n",
    "trending_movie_data= soup.find_all(\"ul\",class_= \"fade-in-articles css-1fv4fhq es0eqmb0\")\n",
    "subheadline_tag = soup.find_all('div', class_='sub-headline css-u6l51d esmf2q80')\n",
    "# By_line=soup.find_all(\"div\",class_=\"css-19pirhg esmf2q85\")\n",
    "\n",
    "# for a in By_line:\n",
    "#     author=a.text.strip()\n",
    "#     if author:\n",
    "#         movie_trending_by_line.append(author)\n",
    "#         print(author)\n",
    "#     else:\n",
    "#         movie_trending_by_line.append(\"No Author Mentioned\")\n",
    "#         print(\"None\")\n",
    "    # print(a.text.strip())\n",
    "\n",
    "\n",
    "# Find the span with the specific class\n",
    "\n",
    "count=0\n",
    "# count_headline\n",
    "for j in subheadline_tag:\n",
    "    subheading_movie=j.find(\"span\")\n",
    "    # print(subheading_movie.get_text(strip=True))\n",
    "    count=count+1\n",
    "    if count==5:\n",
    "        continue\n",
    "    else:    \n",
    "        movie_trending_subheadline.append(subheading_movie.get_text(strip=True))\n",
    "        # print(j.get_text(strip=True))\n",
    "for m in trending_movie_data:\n",
    "    trending_movie_image_div=m.find_all(\"div\",class_=\"css-1d3w5wq\")\n",
    "    trending_movie_headline=m.find_all(\"div\",class_=\"css-1uafqei esmf2q84\")\n",
    "\n",
    "    for k in trending_movie_headline:\n",
    "        heading_movie=k.find(\"a\",class_=\"css-13y5uoc\").text\n",
    "        eyebrow_movie=k.find(\"div\").text\n",
    "        By_line=k.find(\"div\",class_=\"css-19pirhg esmf2q85\")\n",
    "        Date=k.find(\"div\",class_=\"css-1bmnxg7\").text\n",
    "        if By_line:\n",
    "            author=By_line.text.strip()\n",
    "            movie_trending_by_line.append(author)\n",
    "        else:\n",
    "            movie_trending_by_line.append(\"No Author Mentioned\")\n",
    "        # count=count+1\n",
    "        movie_trending_article_date.append(Date)\n",
    "        movie_trending_headline.append(heading_movie)\n",
    "        movie_trending_eyebrow.append(eyebrow_movie)\n",
    "        # movie_trending_by_line.append(By_line.get_text(strip=True))\n",
    "        # print(By_line)\n",
    "\n",
    "\n",
    "\n",
    "    for n in trending_movie_image_div:\n",
    "        image_trending=n.find_all(\"img\",class_=\"css-64wllk\")\n",
    "        for i in image_trending:\n",
    "            movie_image=i.get(\"src\")\n",
    "            movie_trending_images.append(movie_image)\n",
    "\n",
    "    print()\n",
    "print(len(movie_trending_article_date))\n",
    "print(len(movie_trending_images))\n",
    "print(len(movie_trending_eyebrow))\n",
    "print(len(movie_trending_subheadline))\n",
    "print(len(movie_trending_headline))\n",
    "print(len(movie_trending_by_line))\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"Article Headline\": movie_trending_headline,\n",
    "    \"Article SubHeadline\":movie_trending_subheadline,\n",
    "    \"Movie Article Image\": movie_trending_images,\n",
    "    \"Eyebrow Headline\": movie_trending_eyebrow,\n",
    "    \"By\": movie_trending_by_line,\n",
    "    \"Article Date\":movie_trending_article_date\n",
    "})\n",
    "# # print(df)\n",
    "# df.to_csv(\"trending_movies_data1.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "# print(\"CSV file saved as trending_movie_data.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "# Number of pages to scrape\n",
    "pagesToGet = 3\n",
    "upperframe = []\n",
    "\n",
    "for page in range(1, pagesToGet + 1):\n",
    "    print('Processing page:', page)\n",
    "    url = f'https://www.politifact.com/factchecks/?page={page}'\n",
    "    print(url)\n",
    "\n",
    "    try:\n",
    "        page_response = requests.get(url)\n",
    "        page_response.raise_for_status()\n",
    "    except Exception as e:\n",
    "        error_type, error_obj, error_info = sys.exc_info()\n",
    "        print('ERROR FOR LINK:', url)\n",
    "        print(error_type, 'Line:', error_info.tb_lineno)\n",
    "        continue\n",
    "\n",
    "    soup = BeautifulSoup(page_response.text, 'html.parser')\n",
    "    links = soup.find_all('li', attrs={'class': 'o-listicle__item'})\n",
    "    print(\"Items found:\", len(links))\n",
    "\n",
    "    frame = []\n",
    "\n",
    "    for j in links:\n",
    "        try:\n",
    "            statement = j.find(\"div\", attrs={'class': 'm-statement__quote'}).text.strip()\n",
    "\n",
    "            link = \"https://www.politifact.com\"\n",
    "            link += j.find(\"div\", attrs={'class': 'm-statement__quote'}).find('a')['href'].strip()\n",
    "\n",
    "            date = j.find('div', attrs={'class': 'm-statement__body'}).find('footer').text.strip()\n",
    "\n",
    "            source = j.find('div', attrs={'class': 'm-statement__meta'}).find('a').text.strip()\n",
    "\n",
    "            label = j.find('div', attrs={'class': 'm-statement__content'}) \\\n",
    "                .find('img', attrs={'class': 'c-image__original'}).get('alt').strip()\n",
    "\n",
    "            frame.append((statement, link, date, source, label))\n",
    "\n",
    "        except Exception as e:\n",
    "            continue  # Skip this item if any part fails\n",
    "\n",
    "    upperframe.extend(frame)\n",
    "\n",
    "# Create a DataFrame\n",
    "data = pd.DataFrame(upperframe, columns=['Statement', 'Link', 'Date', 'Source', 'Label'])\n",
    "\n",
    "# Display the result\n",
    "print(\"\\nFinal Extracted Data:\")\n",
    "data\n",
    "\n",
    "# Optional: Save to CSV\n",
    "# data.to_csv(\"Politifact_Factchecks.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --->Network Sockets & Connections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes:- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TCP(transmission Control Protocol):-\n",
    "Connection oriented\n",
    "\n",
    "Reliable\n",
    "\n",
    "ordered delivery\n",
    "\n",
    "garented delivery\n",
    "\n",
    "slower\n",
    "\n",
    "Robust error checking and retransmission \n",
    "\n",
    "web browsing email,file transfer etc..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UDP(User Datagram Protocol):\n",
    "Connection less\n",
    "\n",
    "less reliable\n",
    "\n",
    "not order delivery\n",
    "\n",
    "no delivery garenty\n",
    "\n",
    "faster\n",
    "\n",
    "basic error checking and no retransmission\n",
    "\n",
    "online video streaming,gaming ,voip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "host=socket.gethostname()\n",
    "ip=socket.gethostbyname(host)\n",
    "print(host)\n",
    "print(ip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "host=socket.gethostname()\n",
    "ip=socket.gethostbyname('localhost')\n",
    "print(ip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TCP Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile TCP_Server.py\n",
    "import socket as s \n",
    "host='192.168.101.116'\n",
    "port=3600\n",
    "sock=s.socket(s.AF_INET,s.SOCK_STREAM)\n",
    "sock.bind((host,port))\n",
    "sock.listen()#you can pass any num in listen,default is 5\n",
    "print('Server is listening')\n",
    "conn,add=sock.accept()\n",
    "while True:\n",
    "    data=conn.recv(1024).decode()\n",
    "    if data=='CLOSE':\n",
    "        break\n",
    "    print('cLIENT TWO:',data)\n",
    "    msg=input(\"Server Data:\")\n",
    "    conn.send(msg.encode())\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TCP_Server.py\n",
    "\n",
    "import socket as s \n",
    "\n",
    "host = '192.168.101.116'  # Make sure this is the correct IP address for your network\n",
    "port = 3600\n",
    "\n",
    "# Create socket\n",
    "sock = s.socket(s.AF_INET, s.SOCK_STREAM)\n",
    "\n",
    "# Bind socket to address and port\n",
    "sock.bind((host, port))\n",
    "\n",
    "# Listen for incoming connections\n",
    "sock.listen(5)  # Accept up to 5 queued connections\n",
    "print('Server is listening on', host, port)\n",
    "\n",
    "# Accept a connection\n",
    "conn, addr = sock.accept()\n",
    "print(f\"Connected by {addr}\")\n",
    "\n",
    "# Communication loop\n",
    "while True:\n",
    "    data = conn.recv(1024).decode()\n",
    "    if not data:\n",
    "        break\n",
    "    if data.strip().upper() == 'CLOSE':\n",
    "        print(\"Connection closed by client.\")\n",
    "        break\n",
    "    print('CLIENT:', data)\n",
    "    \n",
    "    msg = input(\"Server Data: \")\n",
    "    conn.send(msg.encode())\n",
    "\n",
    "# Close connection\n",
    "conn.close()\n",
    "sock.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile TCP_Client.py\n",
    "import socket as s\n",
    "host='localhost'\n",
    "port=3600\n",
    "csock=s.socket()\n",
    "csock.connect((host,port))\n",
    "while True:\n",
    "    msg=input('client Data:')\n",
    "    csock.send(msg.encode())\n",
    "    if msg=='CLOSE':\n",
    "        break\n",
    "    data=csock.recv(1024).decode()\n",
    "    print('Server Data:',data)\n",
    "csock.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile TCP_Client1.py\n",
    "import socket as s\n",
    "host='localhost'\n",
    "port=3600\n",
    "csock=s.socket()\n",
    "csock.connect((host,port))\n",
    "while True:\n",
    "    msg=int(input('client data'))\n",
    "    csock.send(msg.encode()) \n",
    "    if msg==0:\n",
    "        break\n",
    "    data=csock.recv(1024).decode()\n",
    "    print('Server data:',data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile TCP_Server1.py\n",
    "import socket as s\n",
    "host='192.168.101.114'\n",
    "port=3600\n",
    "csock=s.socket(s.AF_INET,s.SOCK_STREAM)\n",
    "csock.bind((host,port))\n",
    "csock.listen()\n",
    "print('Server is listening')\n",
    "conn,add=csock.accept()\n",
    "msg=csock.recv(1024).decode()\n",
    "msg3=int(msg**3)\n",
    "print('cLIENT TWO:',msg3)\n",
    "conn.send(str(msg).encode())\n",
    "csock.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile UDP_Server.py\n",
    "import socket as s\n",
    "host='localhost'\n",
    "port=3600\n",
    "sock=s.socket(type=s.SOCK_DGRAM)\n",
    "sock.bind((host,port))\n",
    "print('Server is Active')\n",
    "while True:\n",
    "    data,addr=sock.recvfrom(1024)\n",
    "    data=data.decode()\n",
    "    if data=='':\n",
    "        break\n",
    "    print('client:',data)\n",
    "    msg=input('server:').encode()\n",
    "    sock.sendto(msg,addr)\n",
    "sock.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile UDP_Client.py\n",
    "import socket as s\n",
    "host='192.168.101.116'\n",
    "port=3600\n",
    "sock=s.socket(type=s.SOCK_DGRAM)\n",
    "while True:\n",
    "    data=input('Client:').encode()\n",
    "    sock.sendto(data,(host,port))\n",
    "    if data=='':\n",
    "        break\n",
    "    data,addr=sock.recvfrom(1024)\n",
    "    print('Server:',data.decode())\n",
    "sock.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install seaborn\n",
    "pip install scipy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.plotting import scatter_matrix, parallel_coordinates\n",
    "\n",
    "# 1. Load CSV and show basic statistics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Generate sample data\n",
    "np.random.seed(1)\n",
    "data = {\n",
    "    'Duration': np.random.randint(30, 61, 169),\n",
    "    'Pulse': np.random.randint(90, 130, 169),\n",
    "    'Maxpulse': np.random.randint(120, 180, 169),\n",
    "    'Calories': np.round(np.random.uniform(200.0, 500.0, 169), 1)\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('data.csv', index=False)\n",
    "print(\"Sample data.csv file created with 169 rows.\")\n",
    "\n",
    "print(\"Basic Statistics:\\n\", df.describe())\n",
    "\n",
    "# # 2. First and last 5 rows, shape\n",
    "# print(\"\\nFirst 5 Rows:\\n\", df.head())\n",
    "# print(\"\\nLast 5 Rows:\\n\", df.tail())\n",
    "# print(\"\\nShape of DataFrame:\", df.shape)\n",
    "\n",
    "# 3. Correlation Table\n",
    "# correlation = df.corr(numeric_only=True)\n",
    "# print(\"\\nCorrelation Table:\\n\", correlation)\n",
    "# print(\"\\nCorrelation between Duration and Calories:\", correlation.loc[\"Duration\", \"Calories\"])\n",
    "# if correlation.loc[\"Duration\", \"Calories\"] > 0.5:\n",
    "#     print(\"=> Strong Positive Correlation\")\n",
    "# elif correlation.loc[\"Duration\", \"Calories\"] > 0.3:\n",
    "#     print(\"=> Moderate Positive Correlation\")\n",
    "# elif correlation.loc[\"Duration\", \"Calories\"] > 0:\n",
    "#     print(\"=> Weak Positive Correlation\")\n",
    "# else:\n",
    "#     print(\"=> No or Negative Correlation\")\n",
    "# print(correlation)\n",
    "\n",
    "# 4. Check for null values and drop\n",
    "# print(\"\\nNull values in each column:\\n\", df.isnull().sum())\n",
    "# df_cleaned = df.dropna()\n",
    "# print(\"\\nShape after dropping null values:\", df_cleaned.shape)\n",
    "\n",
    "# 5. Scatter Matrix\n",
    "# scatter_matrix(df_cleaned, figsize=(10, 8), diagonal='hist')\n",
    "# plt.suptitle(\"Scatter Matrix\")\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# # 6. Parallel Coordinates\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# parallel_coordinates(df_cleaned[[\"Duration\", \"Pulse\", \"Maxpulse\", \"Calories\"]], 'Duration')\n",
    "# plt.title(\"Parallel Coordinates: Duration vs Others\")\n",
    "# plt.ylabel(\"Values\")\n",
    "# plt.show()\n",
    "\n",
    "# 7. Cross-tabulation: Duration vs Pulse\n",
    "# crosstab = pd.crosstab(df_cleaned['Duration'], df_cleaned['Pulse'])\n",
    "# print(\"\\nCross-tabulation (Duration vs Pulse):\\n\", crosstab)\n",
    "\n",
    "# 8. Detect outliers in Maxpulse using IQR\n",
    "Q1 = df_cleaned['Maxpulse'].quantile(0.25)\n",
    "Q3 = df_cleaned['Maxpulse'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "outliers = df_cleaned[(df_cleaned['Maxpulse'] < (Q1 - 1.5 * IQR)) | (df_cleaned['Maxpulse'] > (Q3 + 1.5 * IQR))]\n",
    "\n",
    "print(f\"\\nOutliers in Maxpulse column:\\n{outliers[['Maxpulse']]}\")\n",
    "print(f\"\\nNumber of outliers in Maxpulse: {outliers.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Sample data for area plot\n",
    "years = [2010, 2011, 2012, 2013, 2014]\n",
    "sales = [200, 300, 450, 350, 500]\n",
    "\n",
    "# Sample data for box plot\n",
    "category1 = [random.randint(1, 50) for _ in range(50)]\n",
    "category2 = [random.randint(25, 75) for _ in range(50)]\n",
    "category3 = [random.randint(50, 100) for _ in range(50)]\n",
    "\n",
    "# Sample data for scatter plot\n",
    "x = [random.uniform(0, 10) for _ in range(50)]\n",
    "y = [random.uniform(0, 10) for _ in range(50)]\n",
    "\n",
    "# Sample data for heatmap\n",
    "data = np.random.rand(5, 5)\n",
    "\n",
    "# Sample data for regression plot\n",
    "height = [160, 165, 170, 175, 180, 185]\n",
    "weight = [60, 65, 70, 75, 80, 85]\n",
    "\n",
    "# 1. Area Plot: Sales over Years\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.fill_between(years, sales, color='skyblue', alpha=0.5)\n",
    "plt.plot(years, sales, marker='o', color='blue')\n",
    "plt.title(\"Sales Trend Over Years\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Sales\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Insight:\n",
    "# The area plot shows a general upward trend in sales with a small dip in 2013.\n",
    "# This indicates overall business growth, with 2012 and 2014 being strong sales years.\n",
    "\n",
    "# 2. Box Plot: Distribution of Categories\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.boxplot([category1, category2, category3], labels=['Category 1', 'Category 2', 'Category 3'])\n",
    "plt.title(\"Box Plot of Categories\")\n",
    "plt.ylabel(\"Values\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Insight:\n",
    "# The box plot shows Category 1 is centered lower with a wider spread.\n",
    "# Category 3 has the highest median and less variation.\n",
    "# Any points outside whiskers represent potential outliers.\n",
    "\n",
    "# 3. Scatter Plot: X vs Y\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(x, y, color='purple', alpha=0.7)\n",
    "plt.title(\"Scatter Plot of X vs Y\")\n",
    "plt.xlabel(\"X values\")\n",
    "plt.ylabel(\"Y values\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Insight:\n",
    "# The scatter plot shows a random spread with no clear correlation.\n",
    "# Points are scattered uniformly suggesting no strong relationship.\n",
    "\n",
    "# 4. Heatmap: Random Data Matrix\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(data, annot=True)\n",
    "plt.title(\"Heatmap of Random Data\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Insight:\n",
    "# The heatmap gives a color-coded view of data intensity.\n",
    "# Red indicates higher values, blue shows lower.\n",
    "# It helps identify hot spots or patterns in the matrix.\n",
    "\n",
    "# 5. Regression Plot: Height vs Weight\n",
    "plt.figure(figsize=(7, 5))\n",
    "sns.regplot(x=height, y=weight, color='green', marker='o')\n",
    "plt.title(\"Regression Plot: Height vs Weight\")\n",
    "plt.xlabel(\"Height (cm)\")\n",
    "plt.ylabel(\"Weight (kg)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Insight:\n",
    "# The regression plot shows a strong positive linear correlation.\n",
    "# As height increases, weight tends to increase steadily.\n",
    "# Indicates a proportional relationship.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# 1. Load the data\n",
    "data = pd.read_csv(\"insurance.csv\")\n",
    "\n",
    "# 2. Preprocessing: Encode categorical features\n",
    "# Using one-hot encoding for categorical columns: sex, smoker, region\n",
    "data = pd.get_dummies(data, columns=['sex', 'smoker', 'region'], drop_first=True)\n",
    "\n",
    "# 3. Split data into features and target\n",
    "X = data.drop(\"charges\", axis=1)  # Independent variables\n",
    "y = data[\"charges\"]               # Target variable\n",
    "\n",
    "# 4. Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 5. Create and train Linear Regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 6. Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# 7. Outputs\n",
    "print(\"Predicted Charges (first 5):\", y_pred[:5])\n",
    "print(\"Coefficients:\", model.coef_)\n",
    "print(\"Intercept:\", model.intercept_)\n",
    "\n",
    "# 8. Mean Squared Error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# 1. Load the dataset\n",
    "df = pd.read_csv(\"car.csv\")\n",
    "\n",
    "# 2. Create new column: Age_car = 2023 - year\n",
    "df['Age_car'] = 2023 - df['year']\n",
    "\n",
    "# 3. Select features (independent variables) and target (dependent variable)\n",
    "X = df[['Age_car', 'Driven_kms', 'Fuel_Type', 'Selling_type', 'Transmission']]\n",
    "y = df['Selling_Price']\n",
    "\n",
    "# 4. Convert categorical features to numeric using one-hot encoding\n",
    "X = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "# 5. Split the data (80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 6. Train a Multiple Linear Regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 7. Predict the values on test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# 8. Print results\n",
    "print(\"Coefficients:\", model.coef_)\n",
    "print(\"Intercept:\", model.intercept_)\n",
    "print(\"Mean Squared Error:\", mean_squared_error(y_test, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
